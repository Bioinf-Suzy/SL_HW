{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3761c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(-1, 784).astype(np.float32) / 255.0\n",
    "X_test = X_test.reshape(-1, 784).astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd8ba0f",
   "metadata": {},
   "source": [
    "gamma: 0.005   | 0.01  | 0.1  | 0.05\n",
    "latent_dim: 50 | 200   | 200  | 200\n",
    "accuracy: 0.096| 0.108 | 0.088| 0.088"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd10e091",
   "metadata": {},
   "source": [
    "Gamma: Controls the smoothness of the RBF kernel. Higher gamma = more local, sensitive to small changes. Smaller gamma = smoother, more global similarity.\n",
    "Latent_dim: Number of random Fourier features. Larger = better approximation of the true RBF kernel, but slower and more memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f813180a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFF latent space shape: (60000, 200)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "gamma = 0.01\n",
    "latent_dim = 200\n",
    "\n",
    "# Define RFF transformer\n",
    "rff = RBFSampler(gamma=gamma, n_components=latent_dim, random_state=42)\n",
    "\n",
    "# Fit on training data\n",
    "rff.fit(X_train)\n",
    "\n",
    "# Transform data\n",
    "X_train_latent = rff.transform(X_train)\n",
    "X_test_latent = rff.transform(X_test)\n",
    "\n",
    "print(f\"RFF latent space shape: {X_train_latent.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87dfa89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed pixel shape: (1, 784)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Train approximate inverse: latent -> pixel\n",
    "inverse_regressor = Ridge(alpha=1.0)\n",
    "inverse_regressor.fit(X_train_latent, X_train)\n",
    "\n",
    "# Example: encode-decode check\n",
    "z_sample = X_train_latent[0:1]\n",
    "x_recon = inverse_regressor.predict(z_sample)\n",
    "\n",
    "print(f\"Reconstructed pixel shape: {x_recon.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e720e4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "noise = np.random.normal(scale=0.1, size=X_train_latent.shape)\n",
    "Z_noisy = X_train_latent + noise\n",
    "# Suppose you have your clean latent codes:\n",
    "Z_clean = X_train_latent  # shape (n_samples, latent_dim)\n",
    "\n",
    "# Create noisy versions:\n",
    "noise = np.random.normal(0, 1, size=Z_clean.shape)\n",
    "Z_noisy = Z_clean + noise\n",
    "\n",
    "# The target is: learn to predict the noise!\n",
    "# Input: Z_noisy\n",
    "# Target: noise\n",
    "\n",
    "denoiser = LinearRegression()\n",
    "denoiser.fit(Z_noisy, noise)\n",
    "\n",
    "print(\"✅ Denoiser is now fitted!\")\n",
    "\n",
    "# Now you can run your loop:\n",
    "z = np.random.normal(size=(1, latent_dim))\n",
    "\n",
    "T=100\n",
    "\n",
    "for t in range(T):\n",
    "    predicted_noise = denoiser.predict(z)\n",
    "    z = z - predicted_noise  # update rule: remove predicted noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f873968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total generated samples: 1000\n"
     ]
    }
   ],
   "source": [
    "Dn = []\n",
    "m = 100  # samples per digit\n",
    "\n",
    "for digit in range(10):\n",
    "    for i in range(m):\n",
    "        # Start with Gaussian latent\n",
    "        z0 = np.random.normal(size=(1, latent_dim))\n",
    "        # (Optional: condition z0 on digit label if your version does this)\n",
    "        zT = z0  # For demonstration — replace with your diffusion steps\n",
    "        x_gen = inverse_regressor.predict(zT)\n",
    "        Dn.append((x_gen.flatten(), digit))\n",
    "\n",
    "print(f\"Total generated samples: {len(Dn)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fc4e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy on generated data: 0.104\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train classifier on real MNIST\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on generated data\n",
    "X_gen = np.array([x for x, _ in Dn])\n",
    "y_gen = np.array([y for _, y in Dn])\n",
    "\n",
    "y_pred = clf.predict(X_gen)\n",
    "\n",
    "print(\"Classifier accuracy on generated data:\", accuracy_score(y_gen, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
